<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Course Project for Practical Machine Learning</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>





</head>

<body>
<h1>Course Project for Practical Machine Learning</h1>

<h2>Overview</h2>

<p>Here are the steps that I took for this project:</p>

<ol>
<li><p><strong>Groom data</strong>  First I selected only the variables/columns that are summary measures) to reduce the number of variables from 160 to 68.  Then I reduced the number of rows to include only the rows that include values for the summary measures. This reduced the number of rows in the training set from from 19622 to 406.  I conducted these two data compression technics in order to allow the model fitting step to run more quickly versus using the full data set.</p></li>
<li><p><strong>Split data</strong> I allocated 75% of the training data set to use for model building. I held back 25% of the training data to do cross validation</p></li>
<li><p><strong>Train the model</strong> I used the Random Forest method to create my model.</p></li>
<li><p><strong>Cross Validate and Estimate Error Rate</strong>  Then I used my model to predict the  outcome for each observation in the validation data set.  I compared my predicted &ldquo;classe&rdquo; values against actual in order to calculate the error rate.  The error rate that I calculated here is a good estimate of the &ldquo;out of sample error&rdquo;.</p></li>
<li><p><strong>Make predictions for test cases</strong> Finally, I used my model to predict outcomes for the 20 test cases.</p></li>
</ol>

<h2>Results</h2>

<p>Here is the code that I used to compress the dataset (step 1 above)</p>

<pre><code>setwd(&quot;~/Desktop&quot;)
fileUrl_train &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;
fileUrl_test&lt;-&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;
download.file(fileUrl_train, destfile = &quot;./data/train.csv&quot;, method = &quot;curl&quot;)
download.file(fileUrl_test, destfile = &quot;./data/test.csv&quot;, method = &quot;curl&quot;)
list.files(&quot;./data&quot;)
train &lt;- read.csv(&quot;./data/train.csv&quot;)
test &lt;- read.csv(&quot;./data/test.csv&quot;)
set.seed(1965)   

sm_train&lt;-train[,c(18:19,21:22,24:25,27:36,50:59,75:83,93:94,96:97,99:100,103:112,131:132,134:135,137:138,141:150,160)]   

sm_test&lt;-test[,c(18:19,21:22,24:25,27:36,50:59,75:83,93:94,96:97,99:100,103:112,131:132,134:135,137:138,141:150,160)]   

index&lt;-!sm_train$max_roll_belt==&quot;NA&quot;
index_2&lt;-!is.na(index)
s_sm_train&lt;-sm_train[index_2,]
sm_test&lt;-sm_test[,-68]
</code></pre>

<p>Here is the code that I used to split the dataset (step 2 above)   </p>

<pre><code>inTrain&lt;-createDataPartition(s_sm_train$classe, p=0.75, list=FALSE)
final_train&lt;-s_sm_train[inTrain,]
final_validation&lt;-s_sm_train[-inTrain,]
</code></pre>

<p>Here is the command that I used to create the model (step 3 above), along with the associated output from that command.</p>

<pre><code>modelFit&lt;-train(classe~.,data=final_train, method=&quot;rf&quot;)
</code></pre>

<pre><code>Random Forest 

307 samples
 67 predictor
  5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 307, 307, 307, 307, 307, 307, ... 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD  
   2    0.7661585  0.7039798  0.03541543   0.04485759
  34    0.7609265  0.6977762  0.03257624   0.04060441
  67    0.7405003  0.6724544  0.03174584   0.03993815

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
</code></pre>

<p>Here are the commands that I used to perform cross validation (step 4 above) and to estimate the error rate, along with the associated output from those command.</p>

<pre><code>pred_final_validation&lt;-predict(modelFit,final_validation)
table(pred_final_validation,final_validation$classe)
</code></pre>

<pre><code>pred_final_validation  A  B  C  D  E
                    A 27  5  0  2  0
                    B  0 13  2  1  2
                    C  0  1 14  0  0
                    D  0  0  1 14  0
                    E  0  0  0  0 17

</code></pre>

<p><strong>Per the matrix above, 85 of the predictions were accurate (e.g. the ones along the diagonal) and 14 were inaccurate.  Thus the error rate on the validation sample is 14/99=14%.  We can use this as an estimate of our Out of Error Sample Rate.</strong></p>

</body>

</html>

